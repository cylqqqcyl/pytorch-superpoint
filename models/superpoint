digraph {
	graph [label=SuperPointNet labelloc=t]
	inp_x [shape=ellipse]
	.1314 [label="conv1a (Conv2d)" shape=box]
	inp_x -> .1314
	.1315 [label="bn1a (BatchNorm2d)" shape=box]
	.1314 -> .1315
	.1316 [label="relu (ReLU)" shape=box]
	.1315 -> .1316
	.1317 [label="conv1b (Conv2d)" shape=box]
	.1316 -> .1317
	.1318 [label="bn1b (BatchNorm2d)" shape=box]
	.1317 -> .1318
	.1319 [label="relu (ReLU)" shape=box]
	.1318 -> .1319
	.1320 [label="pool (MaxPool2d)" shape=box]
	.1319 -> .1320
	.1321 [label="conv2a (Conv2d)" shape=box]
	.1320 -> .1321
	.1322 [label="bn2a (BatchNorm2d)" shape=box]
	.1321 -> .1322
	.1323 [label="relu (ReLU)" shape=box]
	.1322 -> .1323
	.1324 [label="conv2b (Conv2d)" shape=box]
	.1323 -> .1324
	.1325 [label="bn2b (BatchNorm2d)" shape=box]
	.1324 -> .1325
	.1326 [label="relu (ReLU)" shape=box]
	.1325 -> .1326
	.1327 [label="pool (MaxPool2d)" shape=box]
	.1326 -> .1327
	.1328 [label="conv3a (Conv2d)" shape=box]
	.1327 -> .1328
	.1329 [label="bn3a (BatchNorm2d)" shape=box]
	.1328 -> .1329
	.1330 [label="relu (ReLU)" shape=box]
	.1329 -> .1330
	.1331 [label="conv3b (Conv2d)" shape=box]
	.1330 -> .1331
	.1332 [label="bn3b (BatchNorm2d)" shape=box]
	.1331 -> .1332
	.1333 [label="relu (ReLU)" shape=box]
	.1332 -> .1333
	.1334 [label="pool (MaxPool2d)" shape=box]
	.1333 -> .1334
	.1335 [label="conv4a (Conv2d)" shape=box]
	.1334 -> .1335
	.1336 [label="bn4a (BatchNorm2d)" shape=box]
	.1335 -> .1336
	.1337 [label="relu (ReLU)" shape=box]
	.1336 -> .1337
	.1338 [label="conv4b (Conv2d)" shape=box]
	.1337 -> .1338
	.1339 [label="bn4b (BatchNorm2d)" shape=box]
	.1338 -> .1339
	.1339 -> "DAN.inp_5"
	subgraph "cluster_.1340" {
		label="DAN (DAModule)"
		"DAN.inp_5" [shape=ellipse]
		"DAN.inp_5" -> "DAN.position_attention_module.inp_1"
		subgraph "cluster_DAN..37" {
			label="DAN.position_attention_module (PositionAttentionModule)"
			"DAN.position_attention_module.inp_1" [shape=ellipse]
			"DAN.position_attention_module..28" [label="DAN.position_attention_module.cnn (Conv2d)" shape=box]
			"DAN.position_attention_module.inp_1" -> "DAN.position_attention_module..28"
			"DAN.position_attention_module..28" -> "DAN.position_attention_module.pa.inp_queries.1"
			subgraph "cluster_DAN.position_attention_module..29" {
				label="DAN.position_attention_module.pa (ScaledDotProductAttention)"
				"DAN.position_attention_module.pa.inp_queries.1" [shape=ellipse]
				"DAN.position_attention_module.pa..86" [label="DAN.position_attention_module.pa.fc_q (Linear)" shape=box]
				"DAN.position_attention_module.pa.inp_queries.1" -> "DAN.position_attention_module.pa..86"
				"DAN.position_attention_module.pa..87" [label="DAN.position_attention_module.pa.fc_k (Linear)" shape=box]
				"DAN.position_attention_module.pa.inp_queries.1" -> "DAN.position_attention_module.pa..87"
				"DAN.position_attention_module.pa..88" [label="DAN.position_attention_module.pa.fc_v (Linear)" shape=box]
				"DAN.position_attention_module.pa.inp_queries.1" -> "DAN.position_attention_module.pa..88"
				"DAN.position_attention_module.pa..89" [label="DAN.position_attention_module.pa.dropout (Dropout)" shape=box]
				"DAN.position_attention_module.pa.input.53" [label="div softmax matmul" shape=box style=rounded]
				"DAN.position_attention_module.pa..87" -> "DAN.position_attention_module.pa.input.53"
				"DAN.position_attention_module.pa..86" -> "DAN.position_attention_module.pa.input.53"
				"DAN.position_attention_module.pa.input.53" -> "DAN.position_attention_module.pa..89"
				"DAN.position_attention_module.pa..90" [label="DAN.position_attention_module.pa.fc_o (Linear)" shape=box]
				"DAN.position_attention_module.pa.input.55" [label=matmul shape=box style=rounded]
				"DAN.position_attention_module.pa..89" -> "DAN.position_attention_module.pa.input.55"
				"DAN.position_attention_module.pa..88" -> "DAN.position_attention_module.pa.input.55"
				"DAN.position_attention_module.pa.input.55" -> "DAN.position_attention_module.pa..90"
				"DAN.position_attention_module.pa.out_0" [shape=ellipse]
				"DAN.position_attention_module.pa..90" -> "DAN.position_attention_module.pa.out_0"
			}
			"DAN.position_attention_module.out_0" [shape=ellipse]
			"DAN.position_attention_module.pa.out_0" -> "DAN.position_attention_module.out_0"
		}
		"DAN.inp_5" -> "DAN.channel_attention_module.inp_1"
		subgraph "cluster_DAN..38" {
			label="DAN.channel_attention_module (ChannelAttentionModule)"
			"DAN.channel_attention_module.inp_1" [shape=ellipse]
			"DAN.channel_attention_module..23" [label="DAN.channel_attention_module.cnn (Conv2d)" shape=box]
			"DAN.channel_attention_module.inp_1" -> "DAN.channel_attention_module..23"
			"DAN.channel_attention_module..23" -> "DAN.channel_attention_module.pa.inp_queries"
			subgraph "cluster_DAN.channel_attention_module..24" {
				label="DAN.channel_attention_module.pa (SimplifiedScaledDotProductAttention)"
				"DAN.channel_attention_module.pa.inp_queries" [shape=ellipse]
				"DAN.channel_attention_module.pa..80" [label="DAN.channel_attention_module.pa.dropout (Dropout)" shape=box]
				"DAN.channel_attention_module.pa.input.57" [label="div softmax matmul" shape=box style=rounded]
				"DAN.channel_attention_module.pa.inp_queries" -> "DAN.channel_attention_module.pa.input.57"
				"DAN.channel_attention_module.pa.input.57" -> "DAN.channel_attention_module.pa..80"
				"DAN.channel_attention_module.pa..81" [label="DAN.channel_attention_module.pa.fc_o (Linear)" shape=box]
				"DAN.channel_attention_module.pa.input.59" [label=matmul shape=box style=rounded]
				"DAN.channel_attention_module.pa.inp_queries" -> "DAN.channel_attention_module.pa.input.59"
				"DAN.channel_attention_module.pa..80" -> "DAN.channel_attention_module.pa.input.59"
				"DAN.channel_attention_module.pa.input.59" -> "DAN.channel_attention_module.pa..81"
				"DAN.channel_attention_module.pa.out_0" [shape=ellipse]
				"DAN.channel_attention_module.pa..81" -> "DAN.channel_attention_module.pa.out_0"
			}
			"DAN.channel_attention_module.out_0" [shape=ellipse]
			"DAN.channel_attention_module.pa.out_0" -> "DAN.channel_attention_module.out_0"
		}
		"DAN.out_0" [shape=ellipse]
		"inp_DAN.out_0" [label=add shape=box style=rounded]
		"DAN.channel_attention_module.out_0" -> "inp_DAN.out_0"
		"DAN.position_attention_module.out_0" -> "inp_DAN.out_0"
		"inp_DAN.out_0" -> "DAN.out_0"
	}
	.1341 [label="relu (ReLU)" shape=box]
	"DAN.out_0" -> .1341
	.1342 [label="convPa (Conv2d)" shape=box]
	.1341 -> .1342
	.1343 [label="bnPa (BatchNorm2d)" shape=box]
	.1342 -> .1343
	.1344 [label="relu (ReLU)" shape=box]
	.1343 -> .1344
	.1345 [label="convPb (Conv2d)" shape=box]
	.1344 -> .1345
	.1346 [label="bnPb (BatchNorm2d)" shape=box]
	.1345 -> .1346
	.1347 [label="convDa (Conv2d)" shape=box]
	.1341 -> .1347
	.1348 [label="bnDa (BatchNorm2d)" shape=box]
	.1347 -> .1348
	.1349 [label="relu (ReLU)" shape=box]
	.1348 -> .1349
	.1350 [label="convDb (Conv2d)" shape=box]
	.1349 -> .1350
	.1351 [label="bnDb (BatchNorm2d)" shape=box]
	.1350 -> .1351
	out_0 [shape=ellipse]
	inp_out_0 [label="div norm" shape=box style=rounded]
	.1351 -> inp_out_0
	.1346 -> inp_out_0
	inp_out_0 -> out_0
}
